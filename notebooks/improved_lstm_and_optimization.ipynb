{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3a51d9",
   "metadata": {},
   "source": [
    "# Improved LSTM Model and Portfolio Optimization\n",
    "\n",
    "This notebook implements an enhanced LSTM model with better architecture and robust portfolio optimization using modern portfolio theory with ESG constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b588095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import yfinance as yf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"data/processed/MultiIndex_stock_data.csv\", index_col=0)\n",
    "merged_df.index = pd.to_datetime(merged_df.index)\n",
    "esg_scores = pd.read_csv(\"data/raw_data/esg_scores.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28d1fe",
   "metadata": {},
   "source": [
    "## 1. Improved Data Preprocessing\n",
    "\n",
    "Implementing robust data preprocessing with better feature scaling and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(data, lookback=60, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Prepare sequence data with proper padding and scaling\n",
    "    \"\"\"\n",
    "    # Standard scaling for better gradient flow\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Create sequences with proper padding\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - lookback - forecast_horizon + 1):\n",
    "        X.append(scaled_data[i:(i + lookback)])\n",
    "        y.append(scaled_data[i + lookback:i + lookback + forecast_horizon, 0])\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "def create_time_series_cv(n_splits=5):\n",
    "    \"\"\"\n",
    "    Create time series cross-validation splits\n",
    "    \"\"\"\n",
    "    return TimeSeriesSplit(n_splits=n_splits, test_size=int(len(merged_df) * 0.2))\n",
    "\n",
    "# Prepare features for each stock\n",
    "stock_features = {}\n",
    "for ticker in merged_df.columns.levels[0]:\n",
    "    stock_data = merged_df[ticker].copy()\n",
    "    \n",
    "    # Additional features\n",
    "    features = pd.DataFrame({\n",
    "        'price': stock_data['Adj Close'],\n",
    "        'volume': stock_data['Volume'],\n",
    "        'daily_return': stock_data['DailyRet'],\n",
    "        'momentum_20d': stock_data['20DayRet'],\n",
    "        'volatility_20d': stock_data['20DayVol'],\n",
    "        'z_score_ret': stock_data['Z20DayRet'],\n",
    "        'z_score_vol': stock_data['Z20DayVol'],\n",
    "        'esg_score': pd.Series(esg_scores.loc[ticker, 'esg_score'], index=stock_data.index)\n",
    "    })\n",
    "    \n",
    "    # Handle missing data\n",
    "    features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "    stock_features[ticker] = features\n",
    "\n",
    "print(\"Features prepared for all stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f59301",
   "metadata": {},
   "source": [
    "## 2. Enhanced LSTM Model Architecture\n",
    "\n",
    "Implementing a bidirectional LSTM with attention and batch normalization for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25528da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_lstm_model(input_shape, lstm_units=50):\n",
    "    \"\"\"\n",
    "    Create an improved LSTM model with bidirectional layers and attention\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Batch normalization on input\n",
    "    x = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    lstm1 = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n",
    "    lstm1 = layers.BatchNormalization()(lstm1)\n",
    "    lstm1 = layers.Dropout(0.2)(lstm1)\n",
    "    \n",
    "    lstm2 = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(lstm1)\n",
    "    lstm2 = layers.BatchNormalization()(lstm2)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = layers.Dense(1, activation='tanh')(lstm2)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    attention = layers.RepeatVector(lstm_units * 2)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "    \n",
    "    # Merge attention with LSTM output\n",
    "    merged = layers.Multiply()([lstm2, attention])\n",
    "    merged = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(merged)\n",
    "    \n",
    "    # Dense layers with residual connections\n",
    "    dense1 = layers.Dense(32, activation='relu')(merged)\n",
    "    dense1 = layers.BatchNormalization()(dense1)\n",
    "    dense1 = layers.Dropout(0.2)(dense1)\n",
    "    \n",
    "    dense2 = layers.Dense(16, activation='relu')(dense1)\n",
    "    dense2 = layers.BatchNormalization()(dense2)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation='linear')(dense2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile with Huber loss for robustness to outliers\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.Huber(),\n",
    "                 metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train models with cross-validation\n",
    "cv = create_time_series_cv()\n",
    "stock_predictions = {}\n",
    "stock_models = {}\n",
    "\n",
    "for ticker in stock_features.keys():\n",
    "    print(f\"\\nTraining model for {ticker}\")\n",
    "    features = stock_features[ticker].values\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X, y, scaler = prepare_sequence_data(features)\n",
    "    \n",
    "    # Initialize lists for cross-validation results\n",
    "    val_predictions = []\n",
    "    val_indices = []\n",
    "    \n",
    "    # Cross-validation training\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
    "        print(f\"Fold {fold + 1}/5\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_improved_lstm_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Store predictions\n",
    "        val_predictions.extend(model.predict(X_val).flatten())\n",
    "        val_indices.extend(val_idx)\n",
    "    \n",
    "    # Store final model and predictions\n",
    "    stock_models[ticker] = model\n",
    "    stock_predictions[ticker] = pd.Series(\n",
    "        scaler.inverse_transform(np.array(val_predictions).reshape(-1, 1)).flatten(),\n",
    "        index=merged_df.index[60:][val_indices]\n",
    "    )\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_mse = np.mean((stock_predictions[ticker] - features[60:, 0][val_indices]) ** 2)\n",
    "    print(f\"{ticker} - Final MSE: {final_mse:.6f}\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame(stock_predictions)\n",
    "predictions_df.to_csv(\"data/processed/lstm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6b17d",
   "metadata": {},
   "source": [
    "## 3. Portfolio Optimization Setup\n",
    "\n",
    "Prepare the predicted returns and risk metrics for portfolio optimization with ESG constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e00528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate expected returns and covariance matrix\n",
    "returns = predictions_df.pct_change().dropna()\n",
    "exp_returns = returns.mean()\n",
    "cov_matrix = returns.cov()\n",
    "\n",
    "# Add ESG constraints\n",
    "esg_threshold = 0.6  # Minimum ESG score threshold\n",
    "min_esg_weight = 0.4  # Minimum portfolio weight for high ESG stocks\n",
    "\n",
    "# Identify high ESG stocks\n",
    "high_esg_stocks = esg_scores[esg_scores['esg_score'] >= esg_threshold].index\n",
    "\n",
    "def portfolio_stats(weights):\n",
    "    \"\"\"\n",
    "    Calculate portfolio statistics\n",
    "    \"\"\"\n",
    "    portfolio_return = np.sum(exp_returns * weights)\n",
    "    portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    sharpe_ratio = portfolio_return / portfolio_risk\n",
    "    \n",
    "    # Calculate ESG score\n",
    "    portfolio_esg = np.sum(esg_scores['esg_score'] * weights)\n",
    "    \n",
    "    return portfolio_return, portfolio_risk, sharpe_ratio, portfolio_esg\n",
    "\n",
    "def optimize_portfolio(target_return=None):\n",
    "    \"\"\"\n",
    "    Optimize portfolio weights with ESG constraints\n",
    "    \"\"\"\n",
    "    n_assets = len(exp_returns)\n",
    "    \n",
    "    # Initial weights\n",
    "    weights = np.array([1/n_assets] * n_assets)\n",
    "    \n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # Sum of weights = 1\n",
    "        {'type': 'ineq', 'fun': lambda x: np.sum(x[esg_scores.index.isin(high_esg_stocks)]) - min_esg_weight}  # ESG constraint\n",
    "    ]\n",
    "    \n",
    "    if target_return is not None:\n",
    "        constraints.append({\n",
    "            'type': 'eq',\n",
    "            'fun': lambda x: np.sum(exp_returns * x) - target_return\n",
    "        })\n",
    "    \n",
    "    # Bounds for individual stock weights (0-20%)\n",
    "    bounds = tuple((0, 0.2) for asset in range(n_assets))\n",
    "    \n",
    "    # Objective function (minimize volatility)\n",
    "    def objective(weights):\n",
    "        return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    \n",
    "    # Optimize\n",
    "    result = minimize(objective, weights, method='SLSQP',\n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return result.x if result.success else None\n",
    "\n",
    "# Calculate efficient frontier points\n",
    "target_returns = np.linspace(exp_returns.min(), exp_returns.max(), 50)\n",
    "efficient_portfolios = []\n",
    "\n",
    "for target in target_returns:\n",
    "    weights = optimize_portfolio(target)\n",
    "    if weights is not None:\n",
    "        stats = portfolio_stats(weights)\n",
    "        efficient_portfolios.append({\n",
    "            'Return': stats[0],\n",
    "            'Risk': stats[1],\n",
    "            'Sharpe': stats[2],\n",
    "            'ESG Score': stats[3],\n",
    "            'Weights': weights\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d17e62",
   "metadata": {},
   "source": [
    "## 4. Portfolio Analysis and Visualization\n",
    "\n",
    "Analyze the optimized portfolios and create visualizations of the efficient frontier and portfolio weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert efficient portfolios to DataFrame\n",
    "ef_df = pd.DataFrame([{\n",
    "    'Return': p['Return'],\n",
    "    'Risk': p['Risk'],\n",
    "    'Sharpe': p['Sharpe'],\n",
    "    'ESG Score': p['ESG Score']\n",
    "} for p in efficient_portfolios])\n",
    "\n",
    "# Plot efficient frontier\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(ef_df['Risk'], ef_df['Return'], c=ef_df['ESG Score'], \n",
    "           cmap='viridis', s=50)\n",
    "plt.colorbar(label='ESG Score')\n",
    "plt.xlabel('Portfolio Risk (Volatility)')\n",
    "plt.ylabel('Expected Return')\n",
    "plt.title('Efficient Frontier with ESG Scores')\n",
    "plt.show()\n",
    "\n",
    "# Find optimal portfolio (highest Sharpe ratio)\n",
    "optimal_idx = ef_df['Sharpe'].idxmax()\n",
    "optimal_portfolio = efficient_portfolios[optimal_idx]\n",
    "\n",
    "# Plot optimal portfolio weights\n",
    "optimal_weights = pd.Series(\n",
    "    optimal_portfolio['Weights'], \n",
    "    index=exp_returns.index\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "optimal_weights.plot(kind='bar')\n",
    "plt.title('Optimal Portfolio Weights')\n",
    "plt.xlabel('Stocks')\n",
    "plt.ylabel('Weight')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(\"\\nOptimal Portfolio Statistics:\")\n",
    "print(f\"Expected Return: {optimal_portfolio['Return']:.4f}\")\n",
    "print(f\"Risk: {optimal_portfolio['Risk']:.4f}\")\n",
    "print(f\"Sharpe Ratio: {optimal_portfolio['Sharpe']:.4f}\")\n",
    "print(f\"ESG Score: {optimal_portfolio['ESG Score']:.4f}\")\n",
    "\n",
    "# Calculate maximum drawdown\n",
    "def calculate_max_drawdown(returns):\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    rolling_max = cumulative.expanding().max()\n",
    "    drawdowns = cumulative / rolling_max - 1\n",
    "    return drawdowns.min()\n",
    "\n",
    "# Calculate portfolio returns\n",
    "portfolio_returns = returns.dot(optimal_portfolio['Weights'])\n",
    "max_drawdown = calculate_max_drawdown(portfolio_returns)\n",
    "print(f\"Maximum Drawdown: {max_drawdown:.4f}\")\n",
    "\n",
    "# Save optimal portfolio\n",
    "optimal_portfolio_df = pd.DataFrame({\n",
    "    'Weight': optimal_weights,\n",
    "    'ESG Score': esg_scores['esg_score']\n",
    "})\n",
    "optimal_portfolio_df.to_csv('data/processed/optimal_portfolio.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
